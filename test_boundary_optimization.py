#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è¾¹ç•Œä¼˜åŒ–åŠŸèƒ½çš„ä¸“ç”¨è„šæœ¬
"""

import os
import sys
import numpy as np
import xarray as xr
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from dem_interpolation import (
    merge_netcdf_files,
    merge_netcdf_files_optimized,
    visualize_boundary_optimization,
    analyze_boundary_gaps,
    fill_boundary_gaps,
    create_unified_grid
)

def create_test_data_with_gaps():
    """
    åˆ›å»ºåŒ…å«è¾¹ç•Œç¼ºå£çš„æµ‹è¯•æ•°æ®
    """
    print("åˆ›å»ºæµ‹è¯•æ•°æ®...")
    
    # åˆ›å»ºä¸‰ä¸ªæ¨¡æ‹Ÿçš„DEMç“¦ç‰‡æ•°æ®
    test_datasets = []
    
    # ç“¦ç‰‡1: å·¦ä¸Šè§’
    lat1 = np.linspace(24.0, 25.0, 100)
    lon1 = np.linspace(110.0, 111.0, 100)
    elev1 = np.random.normal(1000, 200, (100, 100))
    # æ·»åŠ ä¸€äº›åœ°å½¢ç‰¹å¾
    x1, y1 = np.meshgrid(lon1, lat1)
    elev1 += 300 * np.sin(x1 * 5) * np.cos(y1 * 5)
    
    ds1 = xr.Dataset({
        'elevation': (('lat', 'lon'), elev1)
    }, coords={'lat': lat1, 'lon': lon1})
    
    # ç“¦ç‰‡2: å³ä¸Šè§’ï¼ˆä¸ç“¦ç‰‡1æœ‰å°ç¼ºå£ï¼‰
    lat2 = np.linspace(24.0, 25.0, 100)
    lon2 = np.linspace(111.02, 112.02, 100)  # æ•…æ„ç•™0.02åº¦ç¼ºå£
    elev2 = np.random.normal(1200, 150, (100, 100))
    x2, y2 = np.meshgrid(lon2, lat2)
    elev2 += 200 * np.sin(x2 * 3) * np.cos(y2 * 4)
    
    ds2 = xr.Dataset({
        'elevation': (('lat', 'lon'), elev2)
    }, coords={'lat': lat2, 'lon': lon2})
    
    # ç“¦ç‰‡3: ä¸‹æ–¹ï¼ˆä¸ä¸Šé¢ä¸¤ä¸ªéƒ½æœ‰ç¼ºå£ï¼‰
    lat3 = np.linspace(22.98, 23.98, 100)  # æ•…æ„ç•™0.02åº¦ç¼ºå£
    lon3 = np.linspace(110.5, 111.5, 100)
    elev3 = np.random.normal(800, 180, (100, 100))
    x3, y3 = np.meshgrid(lon3, lat3)
    elev3 += 400 * np.exp(-((x3-111)**2 + (y3-23.5)**2) * 10)
    
    ds3 = xr.Dataset({
        'elevation': (('lat', 'lon'), elev3)
    }, coords={'lat': lat3, 'lon': lon3})
    
    test_datasets = [ds1, ds2, ds3]
    
    print(f"åˆ›å»ºäº† {len(test_datasets)} ä¸ªæµ‹è¯•æ•°æ®é›†")
    for i, ds in enumerate(test_datasets):
        lat_range = (ds.lat.min().values, ds.lat.max().values)
        lon_range = (ds.lon.min().values, ds.lon.max().values)
        print(f"  æ•°æ®é›† {i+1}: çº¬åº¦ {lat_range[0]:.3f}Â°-{lat_range[1]:.3f}Â°, "
              f"ç»åº¦ {lon_range[0]:.3f}Â°-{lon_range[1]:.3f}Â°")
    
    return test_datasets

def test_boundary_gap_filling():
    """
    æµ‹è¯•è¾¹ç•Œç¼ºå¤±å€¼å¡«å……ç®—æ³•
    """
    print("\n=== æµ‹è¯•è¾¹ç•Œç¼ºå¤±å€¼å¡«å…… ===")
    
    # åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼çš„æµ‹è¯•æ•°æ®
    test_data = np.random.normal(1000, 200, (50, 50))
    
    # äººå·¥æ·»åŠ ä¸€äº›è¾¹ç•Œç¼ºå¤±å€¼
    test_data[20:25, 10:15] = np.nan  # ä¸­é—´åŒºåŸŸ
    test_data[0:5, :] = np.nan        # é¡¶éƒ¨è¾¹ç•Œ
    test_data[:, 45:50] = np.nan      # å³ä¾§è¾¹ç•Œ
    test_data[10:15, 0:5] = np.nan    # å·¦ä¾§å°å—
    
    print(f"åŸå§‹ç¼ºå¤±å€¼æ•°é‡: {np.sum(np.isnan(test_data))}")
    
    # æµ‹è¯•ä¸åŒå¡«å……æ–¹æ³•
    methods = ['linear', 'nearest']
    for method in methods:
        print(f"\næµ‹è¯• {method} å¡«å……æ–¹æ³•:")
        filled_data = fill_boundary_gaps(test_data.copy(), method=method, max_distance=5)
        remaining_nans = np.sum(np.isnan(filled_data))
        filled_count = np.sum(np.isnan(test_data)) - remaining_nans
        print(f"  å¡«å……äº† {filled_count} ä¸ªç¼ºå¤±å€¼")
        print(f"  å‰©ä½™ç¼ºå¤±å€¼: {remaining_nans}")

def test_boundary_optimization_pipeline():
    """
    æµ‹è¯•å®Œæ•´çš„è¾¹ç•Œä¼˜åŒ–æµç¨‹
    """
    print("\n=== æµ‹è¯•è¾¹ç•Œä¼˜åŒ–å®Œæ•´æµç¨‹ ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_datasets = create_test_data_with_gaps()
    
    # åˆ†æè¾¹ç•Œç¼ºå£
    gaps_analysis = analyze_boundary_gaps(test_datasets, resolution=0.01)
    
    # ä¼ ç»Ÿåˆå¹¶ï¼ˆä¸ä¼˜åŒ–ï¼‰
    print("\n1. ä¼ ç»Ÿåˆå¹¶æ–¹æ³•:")
    try:
        traditional_merged = xr.merge(test_datasets)
        print("âœ“ ä¼ ç»Ÿåˆå¹¶æˆåŠŸ")
    except Exception as e:
        print(f"âœ— ä¼ ç»Ÿåˆå¹¶å¤±è´¥: {e}")
        # ä½¿ç”¨åŸºç¡€çš„ç»Ÿä¸€ç½‘æ ¼æ–¹æ³•
        traditional_merged = create_basic_unified_grid(test_datasets)
    
    # ä¼˜åŒ–åˆå¹¶
    print("\n2. è¾¹ç•Œä¼˜åŒ–åˆå¹¶æ–¹æ³•:")
    optimized_merged = create_unified_grid(test_datasets)
    
    # æ¯”è¾ƒç»“æœ
    if traditional_merged and optimized_merged:
        print("\n3. ç»“æœå¯¹æ¯”:")
        
        trad_valid = np.sum(~np.isnan(traditional_merged['elevation'].values))
        opt_valid = np.sum(~np.isnan(optimized_merged['elevation'].values))
        
        print(f"ä¼ ç»Ÿæ–¹æ³•æœ‰æ•ˆç‚¹æ•°: {trad_valid:,}")
        print(f"ä¼˜åŒ–æ–¹æ³•æœ‰æ•ˆç‚¹æ•°: {opt_valid:,}")
        print(f"æ”¹å–„ç‚¹æ•°: {opt_valid - trad_valid:,}")
        
        if opt_valid > trad_valid:
            improvement = (opt_valid - trad_valid) / trad_valid * 100
            print(f"æ”¹å–„ç¨‹åº¦: {improvement:.2f}%")
        
        # å¯è§†åŒ–å¯¹æ¯”ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            visualize_boundary_optimization(
                traditional_merged, optimized_merged, 
                save_path='boundary_optimization_test.png'
            )
        except Exception as e:
            print(f"å¯è§†åŒ–å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ˜¾ç¤ºé—®é¢˜ï¼‰: {e}")
    
    return gaps_analysis, traditional_merged, optimized_merged

def create_basic_unified_grid(datasets):
    """
    åˆ›å»ºåŸºç¡€çš„ç»Ÿä¸€ç½‘æ ¼ï¼ˆä¸è¿›è¡Œè¾¹ç•Œä¼˜åŒ–ï¼‰
    """
    # æ”¶é›†æ‰€æœ‰åæ ‡
    all_lats = []
    all_lons = []
    
    for ds in datasets:
        all_lats.extend(ds.lat.values)
        all_lons.extend(ds.lon.values)
    
    unique_lats = np.sort(np.unique(all_lats))
    unique_lons = np.sort(np.unique(all_lons))
    
    # åˆ›å»ºç»“æœæ•°ç»„
    result_data = np.full((len(unique_lats), len(unique_lons)), np.nan)
    
    # ç®€å•å¡«å……æ•°æ®
    for ds in datasets:
        elevation = ds['elevation']
        ds_lats = ds.lat.values
        ds_lons = ds.lon.values
        
        for j, lat in enumerate(ds_lats):
            for k, lon in enumerate(ds_lons):
                lat_idx = np.argmin(np.abs(unique_lats - lat))
                lon_idx = np.argmin(np.abs(unique_lons - lon))
                
                if not np.isnan(elevation.values[j, k]):
                    result_data[lat_idx, lon_idx] = elevation.values[j, k]
    
    return xr.Dataset({
        'elevation': (('lat', 'lon'), result_data)
    }, coords={'lat': unique_lats, 'lon': unique_lons})

def test_real_data_if_available():
    """
    å¦‚æœæœ‰çœŸå®æ•°æ®ï¼Œæµ‹è¯•è¾¹ç•Œä¼˜åŒ–åŠŸèƒ½
    """
    print("\n=== æµ‹è¯•çœŸå®æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰===")
    
    netcdf_dir = Path(r'h:\data\DEM\netcdf_output')
    
    if netcdf_dir.exists():
        netcdf_files = list(netcdf_dir.glob('*.nc'))
        
        if len(netcdf_files) >= 2:
            print(f"æ‰¾åˆ° {len(netcdf_files)} ä¸ªNetCDFæ–‡ä»¶")
            
            # é€‰æ‹©å‰å‡ ä¸ªæ–‡ä»¶è¿›è¡Œæµ‹è¯•
            test_files = [str(f) for f in netcdf_files[:4]]  # æœ€å¤šæµ‹è¯•4ä¸ªæ–‡ä»¶
            
            print("æµ‹è¯•æ–‡ä»¶:")
            for f in test_files:
                print(f"  - {Path(f).name}")
            
            try:
                # ä¼ ç»Ÿåˆå¹¶
                print("\nä¼ ç»Ÿåˆå¹¶æ–¹æ³•:")
                traditional_result = merge_netcdf_files(
                    test_files, 
                    'test_traditional_merge.nc'
                )
                
                # ä¼˜åŒ–åˆå¹¶
                print("\nä¼˜åŒ–åˆå¹¶æ–¹æ³•:")
                optimized_result = merge_netcdf_files_optimized(
                    test_files, 
                    'test_optimized_merge.nc',
                    enable_boundary_optimization=True
                )
                
                # å¯¹æ¯”ç»“æœ
                if traditional_result and optimized_result:
                    print("\n=== çœŸå®æ•°æ®æµ‹è¯•ç»“æœ ===")
                    
                    trad_elev = traditional_result['elevation']
                    opt_elev = optimized_result['elevation']
                    
                    trad_valid = np.sum(~np.isnan(trad_elev.values))
                    opt_valid = np.sum(~np.isnan(opt_elev.values))
                    
                    print(f"ä¼ ç»Ÿæ–¹æ³•æœ‰æ•ˆç‚¹: {trad_valid:,}")
                    print(f"ä¼˜åŒ–æ–¹æ³•æœ‰æ•ˆç‚¹: {opt_valid:,}")
                    
                    if opt_valid > trad_valid:
                        improvement = (opt_valid - trad_valid) / trad_valid * 100
                        print(f"è¾¹ç•Œä¼˜åŒ–æ”¹å–„: {improvement:.2f}%")
                        print("âœ“ è¾¹ç•Œä¼˜åŒ–åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼")
                    else:
                        print("- æ­¤æ•°æ®é›†å¯èƒ½æ²¡æœ‰æ˜æ˜¾çš„è¾¹ç•Œç¼ºå£é—®é¢˜")
                
            except Exception as e:
                print(f"çœŸå®æ•°æ®æµ‹è¯•å¤±è´¥: {e}")
        else:
            print(f"NetCDFæ–‡ä»¶æ•°é‡ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘2ä¸ªï¼Œæ‰¾åˆ°{len(netcdf_files)}ä¸ªï¼‰")
    else:
        print("NetCDFç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡çœŸå®æ•°æ®æµ‹è¯•")

def main():
    """
    è¿è¡Œæ‰€æœ‰è¾¹ç•Œä¼˜åŒ–æµ‹è¯•
    """
    print("=== DEMè¾¹ç•Œä¼˜åŒ–åŠŸèƒ½æµ‹è¯• ===")
    
    try:
        # æµ‹è¯•1: è¾¹ç•Œç¼ºå¤±å€¼å¡«å……ç®—æ³•
        test_boundary_gap_filling()
        
        # æµ‹è¯•2: å®Œæ•´çš„è¾¹ç•Œä¼˜åŒ–æµç¨‹
        gaps_analysis, traditional, optimized = test_boundary_optimization_pipeline()
        
        # æµ‹è¯•3: çœŸå®æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        test_real_data_if_available()
        
        print("\n" + "="*60)
        print("ğŸ‰ è¾¹ç•Œä¼˜åŒ–åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
        print("ä¸»è¦æ”¹è¿›:")
        print("- âœ“ è¾¹ç•Œç¼ºå¤±å€¼æ™ºèƒ½å¡«å……")
        print("- âœ“ æ•°æ®é›†è¾¹ç•Œå¹³æ»‘å¤„ç†")
        print("- âœ“ é‡å åŒºåŸŸåŠ æƒå¹³å‡")
        print("- âœ“ å¤šå±‚æ¬¡ç¼ºå£å¡«å……ç­–ç•¥")
        print("\nä½¿ç”¨æ–°çš„ä¼˜åŒ–åŠŸèƒ½é‡æ–°è¿è¡Œæ‰¹é‡å¤„ç†ï¼Œ")
        print("è¾¹ç•Œç¼ºå¤±å€¼é—®é¢˜å°†å¾—åˆ°æ˜¾è‘—æ”¹å–„ï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…")

if __name__ == "__main__":
    main()